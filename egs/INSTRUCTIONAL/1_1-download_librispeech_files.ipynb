{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1: Download `librispeech` files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a free dataset consisting of readings from books available at `project gutenberg`.\n",
    "\n",
    "The language models and lexicons are explained [here](http://www.openslr.org/12/).\n",
    "\n",
    "Note: You do **not** need to download them yourself.  The scripts below will automatically download the necessary files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location to download raw audio\n",
    "data=${KALDI_INSTRUCTIONAL_PATH}/raw_data\n",
    "mkdir $data\n",
    "\n",
    "# base url for downloads\n",
    "data_url=www.openslr.org/resources/12\n",
    "lm_url=www.openslr.org/resources/11\n",
    "\n",
    "# source files with path information\n",
    ". ./path.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The audio files are explained [here](http://www.openslr.org/12/).  \n",
    "\n",
    "There are two sets of audio: `clean` and `other`.  `clean` is a subset of the audio that is very clearly articulated and \"easier\" to run through `ASR`.  `other` is a subset of data that is much more difficult to run through `ASR`.  There are also three different sized training sets: `100 hrs`, `360 hrs`, and `500 hrs`.  \n",
    "\n",
    "We will do all of our training on `train-clean-100`, and will test on *both* `test-clean` and `test-other`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![audio_splits](notebook_images/1_1-librispeech_audio_splits.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below will download the following audio subsets into the directory `INSTRUCTIONAL/raw_data`:\n",
    " - `train-clean-100`\n",
    " - `dev-clean`\n",
    " - `dev-other`\n",
    " - `test-clean`\n",
    " - `test-other`\n",
    "\n",
    "**Note:** This step could take up to `3 hrs` to complete (depending on your internet connection speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in dev-clean test-clean dev-other test-other train-clean-100; do\n",
    "  local/download_and_untar.sh $data $data_url $part\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading `language model`s and `lexicon`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the other files needed, `language model`s and `lexicon`, have already been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lm_files](notebook_images/1_1-librispeech_data_files.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`librispeech-lm-norm.txt.gz` is a compressed file of the `text` used to build the language models. <br>\n",
    "`librispeech-lexicon.txt` is a file that contains all the words in the `ASR` vocabulary and their pronunciations. <br>\n",
    "`3-gram.arpa.gz` is a compressed `3-gram` `language model`. <br>\n",
    "`3-gram.pruned.1e-7.arpa.gz` and `3gram.pruned.3e-7.arpa.gz` are `prune`d versions of `3-gram.arpa.gz`. <br>\n",
    "`4-gram.arpa.gz` is a `4-gram` `language model`. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below will download these files into the directory `INSTRUCTIONAL/raw_data`.\n",
    "\n",
    "**Note:** This step could take up to `1 hr` to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local/download_lm.sh $lm_url $data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
