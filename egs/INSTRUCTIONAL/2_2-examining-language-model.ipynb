{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2: Examining language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ARPA` (and `iARPA`) format is very interpretable.  If you haven't yet done so, read this short [blog post](https://cmusphinx.github.io/wiki/arpaformat/) for more information on how to interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iARPA\n",
      "\n",
      "\\data\\\n",
      "ngram 1=\t15\n",
      "ngram 2=\t33\n",
      "\n",
      "\\1-grams:\n",
      "-1.826075\t<s>\t-0.698970\n",
      "-0.583037\tthe\t-0.767686\n",
      "-1.348954\tmouse\t-0.352183\n",
      "-0.951014\tate\t-1.176091\n",
      "-1.348954\tcheese\t-0.544068\n",
      "-1.282007\t</s>\t-0.845098\n",
      "-1.282007\tcat\t-0.397940\n",
      "-1.085712\tand\t-1.041393\n",
      "-1.282007\tdog\t-0.397940\n",
      "-1.282007\tlion\t-0.477121\n",
      "-1.348954\ttyrannosaurus\t-0.778151\n",
      "-1.348954\trex\t-0.544068\n",
      "-1.826075\thuman\t-0.301030\n",
      "-1.826075\tshot\t-0.301030\n",
      "-0.951014\t<unk>\n",
      "\\2-grams:\n",
      "-0.698970\t<s> <s>\n",
      "-0.221849\t<s> the\n",
      "-0.913814\tthe mouse\n",
      "-0.913814\tthe cheese\n",
      "-0.834633\tthe cat\n",
      "-0.834633\tthe dog\n",
      "-0.834633\tthe lion\n",
      "-0.913814\tthe tyrannosaurus\n",
      "-1.612784\tthe human\n",
      "-0.954243\tmouse the\n",
      "-0.954243\tmouse ate\n",
      "-0.954243\tmouse </s>\n",
      "-0.653213\tmouse and\n",
      "-0.029963\tate the\n",
      "-0.845098\tcheese </s>\n",
      "-0.243038\tcheese and\n",
      "-1.000000\tcat the\n",
      "-0.698970\tcat ate\n",
      "-1.000000\tcat </s>\n",
      "-0.698970\tcat and\n",
      "-0.041393\tand the\n",
      "-1.000000\tdog the\n",
      "-0.522879\tdog ate\n",
      "-1.000000\tdog </s>\n",
      "-1.000000\tdog and\n",
      "-0.352183\tlion ate\n",
      "-0.954243\tlion </s>\n",
      "-0.954243\tlion and\n",
      "-0.079181\ttyrannosaurus rex\n",
      "-0.243038\trex ate\n",
      "-0.845098\trex </s>\n",
      "-0.301030\thuman shot\n",
      "-0.301030\tshot the\n",
      "\\end\\\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat resource_files/language_model/animal_lm-2_gram.iarpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `PyNLPl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [`PyNLPl`](http://pynlpl.readthedocs.io/en/latest/) (pronounced \"pineapple\") package in `python` to examine our language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pynlpl.lm.lm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6d238cd0ee42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpynlpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named pynlpl.lm.lm"
     ]
    }
   ],
   "source": [
    "import pynlpl.lm.lm as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading in `.iARPA` files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ARPALanguageModel()` will import an existing **`iARPA`** formatted language model.\n",
    "\n",
    "**Note:** Recall that in the last notebook we had to run a quick `sed` command over the `.iarpa` format because there were times where the whitespace between a probability and the `1-gram` was a `\" \"` instead of a `\\t`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to parse ARPA LM line: iARPA\n",
      "Adding to LM: (u'<s>',)\t-1.826075\t-0.69897\n",
      "Adding to LM: (u'the',)\t-0.583037\t-0.767686\n",
      "Adding to LM: (u'mouse',)\t-1.348954\t-0.352183\n",
      "Adding to LM: (u'ate',)\t-0.951014\t-1.176091\n",
      "Adding to LM: (u'cheese',)\t-1.348954\t-0.544068\n",
      "Adding to LM: (u'</s>',)\t-1.282007\t-0.845098\n",
      "Adding to LM: (u'cat',)\t-1.282007\t-0.39794\n",
      "Adding to LM: (u'and',)\t-1.085712\t-1.041393\n",
      "Adding to LM: (u'dog',)\t-1.282007\t-0.39794\n",
      "Adding to LM: (u'lion',)\t-1.282007\t-0.477121\n",
      "Adding to LM: (u'tyrannosaurus',)\t-1.348954\t-0.778151\n",
      "Adding to LM: (u'rex',)\t-1.348954\t-0.544068\n",
      "Adding to LM: (u'human',)\t-1.826075\t-0.30103\n",
      "Adding to LM: (u'shot',)\t-1.826075\t-0.30103\n",
      "Adding to LM: (u'<unk>',)\t-0.951014\n",
      "Adding to LM: (u'<s>', u'<s>')\t-0.69897\n",
      "Adding to LM: (u'<s>', u'the')\t-0.221849\n",
      "Adding to LM: (u'the', u'mouse')\t-0.913814\n",
      "Adding to LM: (u'the', u'cheese')\t-0.913814\n",
      "Adding to LM: (u'the', u'cat')\t-0.834633\n",
      "Adding to LM: (u'the', u'dog')\t-0.834633\n",
      "Adding to LM: (u'the', u'lion')\t-0.834633\n",
      "Adding to LM: (u'the', u'tyrannosaurus')\t-0.913814\n",
      "Adding to LM: (u'the', u'human')\t-1.612784\n",
      "Adding to LM: (u'mouse', u'the')\t-0.954243\n",
      "Adding to LM: (u'mouse', u'ate')\t-0.954243\n",
      "Adding to LM: (u'mouse', u'</s>')\t-0.954243\n",
      "Adding to LM: (u'mouse', u'and')\t-0.653213\n",
      "Adding to LM: (u'ate', u'the')\t-0.029963\n",
      "Adding to LM: (u'cheese', u'</s>')\t-0.845098\n",
      "Adding to LM: (u'cheese', u'and')\t-0.243038\n",
      "Adding to LM: (u'cat', u'the')\t-1.0\n",
      "Adding to LM: (u'cat', u'ate')\t-0.69897\n",
      "Adding to LM: (u'cat', u'</s>')\t-1.0\n",
      "Adding to LM: (u'cat', u'and')\t-0.69897\n",
      "Adding to LM: (u'and', u'the')\t-0.041393\n",
      "Adding to LM: (u'dog', u'the')\t-1.0\n",
      "Adding to LM: (u'dog', u'ate')\t-0.522879\n",
      "Adding to LM: (u'dog', u'</s>')\t-1.0\n",
      "Adding to LM: (u'dog', u'and')\t-1.0\n",
      "Adding to LM: (u'lion', u'ate')\t-0.352183\n",
      "Adding to LM: (u'lion', u'</s>')\t-0.954243\n",
      "Adding to LM: (u'lion', u'and')\t-0.954243\n",
      "Adding to LM: (u'tyrannosaurus', u'rex')\t-0.079181\n",
      "Adding to LM: (u'rex', u'ate')\t-0.243038\n",
      "Adding to LM: (u'rex', u'</s>')\t-0.845098\n",
      "Adding to LM: (u'human', u'shot')\t-0.30103\n",
      "Adding to LM: (u'shot', u'the')\t-0.30103\n"
     ]
    }
   ],
   "source": [
    "bi_gram_lm = lm.ARPALanguageModel(\n",
    "    filename=\"resource_files/language_model/animal_lm-2_gram.iarpa\",\n",
    "    base_e=False,  # this will keep the log probabilities in `base 10` so that they match up with the original file\n",
    "    debug=True     # this argument will allow you to more easily see how the data is stored in the object\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that each `n-gram` is stored as a `<tuple>`, even `1-gram`s ==> `([word],)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking up **existing** `n-gram`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.ngrams` contains all the of `n-gram`s **present** in our language model.  We can access either:\n",
    " - the probability ==> `.prob()`\n",
    " - the backoff probability ==> `.backoff()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.282007, -0.39794)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_lm.ngrams.prob((\"dog\",)), bi_gram_lm.ngrams.backoff((\"dog\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm this by double-checking the values in the original `.iarpa` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.282007\tdog\t-0.397940\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat resource_files/language_model/animal_lm-2_gram.iarpa | grep -P \"\\tdog\\t\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.834633, 0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_lm.ngrams.prob((\"the\", \"dog\")), bi_gram_lm.ngrams.backoff((\"the\", \"dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.834633\tthe dog\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat resource_files/language_model/animal_lm-2_gram.iarpa | grep -P \"the dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we try to lookup an `n-gram` that does **not** exist in the language model explicitly, we get a `KeyError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram ('human', 'ate') doesn't exist in language model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bi_gram_lm.ngrams.prob((\"human\", \"ate\"))\n",
    "except Exception as e:\n",
    "    print(\"n-gram {} doesn't exist in language model\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram ('the', 'dog', 'ate') doesn't exist in language model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bi_gram_lm.ngrams.prob((\"the\", \"dog\", \"ate\"))\n",
    "except Exception as e:\n",
    "    print(\"n-gram {} doesn't exist in language model\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating new `n-gram` probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two cases where this will occur:\n",
    " 1. The `n-gram` is of the size of the language model **but** this particular `n-gram` is **not found** in the language model.\n",
    " 2. The `n-gram` is **larger** than that of the language model.  In other words, you want the probability of a `3-gram`, but your language model is only made up of `2-gram`s.\n",
    " \n",
    "In both cases, we can use `.score()`.  To score a new `n-gram`, provide that `n-gram` as a `<tuple>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `n-gram` is **not present** in language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases like these, we need to access `backoff` probabilities, which are designed precisely for this purpose.  You'll notice that in our `2-gram` language model, backoff probabilities exist for `1-gram`s only.  It is the number that comes **after** the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\1-grams:\r\n",
      "-1.826075\t<s>\t-0.698970\r\n",
      "-0.583037\tthe\t-0.767686\r\n",
      "-1.348954\tmouse\t-0.352183\r\n",
      "-0.951014\tate\t-1.176091\r\n",
      "-1.348954\tcheese\t-0.544068\r\n",
      "-1.282007\t</s>\t-0.845098\r\n",
      "-1.282007\tcat\t-0.397940\r\n",
      "-1.085712\tand\t-1.041393\r\n",
      "-1.282007\tdog\t-0.397940\r\n",
      "-1.282007\tlion\t-0.477121\r\n",
      "-1.348954\ttyrannosaurus\t-0.778151\r\n",
      "-1.348954\trex\t-0.544068\r\n",
      "-1.826075\thuman\t-0.301030\r\n",
      "-1.826075\tshot\t-0.301030\r\n",
      "-0.951014\t<unk>\r\n"
     ]
    }
   ],
   "source": [
    "cat resource_files/language_model/animal_lm-2_gram.iarpa | grep -A15 -E \"1-grams\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we wanted to find the probability of \"human ate\", it would be calculated as:\n",
    "\n",
    "$p(human\\_ate) = p(human) + p(ate|human) = p(human) + p(UNK) + bWt(human)$\n",
    "\n",
    "**Note:** Remember because our probabilities are in `negative log`-space, we will **add** instead of **multiply**.  And since all of our probabilities will be `negative`, the **closer** the probability is to `0`, the \"more likely\" the word/sequence is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.078119"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_lm.score((\"human\", \"ate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm this by doing the calculations ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.078119"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_lm.ngrams.prob((\"human\",)) + \\\n",
    "bi_gram_lm.ngrams.prob((\"<unk>\",)) + \\\n",
    "bi_gram_lm.ngrams.backoff((\"human\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you forget to enter the `n-gram` as a `<tuple>`, the `<string>` will be considered an `n-gram` of **characters**, **none of which** will be present in the language model, so it will be equal to $p(UNK) * len(string)$\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.559126"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_lm.score(\"human ate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.559126"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = 0\n",
    "for i in \"human ate\":\n",
    "    result += bi_gram_lm.scoreword(i)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `n-gram` is **larger** than language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get the probability of the sequence, `\"the dog ate\"` using our `2-gram` language model, it will be calculated as follows:\n",
    "\n",
    "$p(the\\_dog\\_ate) = p(the) + p(dog|the) + p(ate|dog)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9405489999999999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_lm.score((\"the\", \"dog\", \"ate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can again confirm this by doing the calculation ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9405489999999999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_lm.ngrams.prob((\"the\",)) + \\\n",
    "bi_gram_lm.ngrams.prob((\"the\", \"dog\")) + \\\n",
    "bi_gram_lm.ngrams.prob((\"dog\", \"ate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if any one of the `n-grams` we use to compose our sequence is **not** present in our language model, we again need to utilize the `backoff` probabilities.\n",
    "\n",
    "$p(the\\_triceratops\\_ate) = p(the) + p(triceratops|the) + p(ate|triceratops) = p(the) + p(UNK) + bWt(the) + p(ate) + bWt(triceratops)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.252751"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_lm.score((\"the\", \"triceratops\", \"ate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.252751"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_lm.ngrams.prob((\"the\",)) + \\\n",
    "bi_gram_lm.ngrams.prob((\"<unk>\",)) + \\\n",
    "bi_gram_lm.ngrams.backoff((\"the\",)) + \\\n",
    "bi_gram_lm.ngrams.prob((\"ate\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing probabilities\n",
    "\n",
    "Now that we have the tools, let's look at how \"likely\" particular sequences of words will be given our language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our animal corpus provided us with a \"hierarchical\" understanding of the food chain.  For example, our `\"mouse\"` could only eat **one** thing while our `\"lion\"` ate **four** things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mouse ate the cheese\r\n"
     ]
    }
   ],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt | grep \"mouse ate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the lion ate the cheese and the lion ate the mouse and the lion ate the cat and the lion ate the dog\r\n"
     ]
    }
   ],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt | grep \"lion ate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so intuitively, the `2-gram` `\"mouse ate\"` should be **four times less likely** than `\"lion ate\"`.\n",
    "\n",
    "**Note:** Remember that these probabilities are in `log base 10` space, so we need to do a quick conversion in order to see the expected ratio between the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11111098560477115, 0.4444439512937877)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**bi_gram_lm.ngrams.prob((\"mouse\", \"ate\")), 10**bi_gram_lm.ngrams.prob((\"lion\", \"ate\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as the `\"lion\"` is relatively \"high up\" in our food chain, it is eaten by less things than the `\"mouse\"` is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tyrannosaurus rex ate the cheese and the tyrannosaurus rex ate the cat and the tyrannosaurus rex ate the dog and the tyrannosaurus rex ate the lion\r\n"
     ]
    }
   ],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt | grep \"ate the lion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat ate the cheese and the cat ate the mouse\r\n",
      "the dog ate the cheese and the dog ate the mouse and the dog ate the cat\r\n",
      "the lion ate the cheese and the lion ate the mouse and the lion ate the cat and the lion ate the dog\r\n"
     ]
    }
   ],
   "source": [
    "cat resource_files/language_model/animal_corpus.txt | grep \"ate the mouse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we should might the probability of `\"ate the lion\"` to be **three times less likely** than `\"ate the mouse`\".  And since we are dealing with a `2-gram` language model, we will need to do some probability calculations this time (instead of being able to simply \"look up\" the probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.015289384411767658, 0.012741160894919555)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**bi_gram_lm.score((\"ate\", \"the\", \"lion\")), 10**bi_gram_lm.score((\"ate\", \"the\", \"mouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is clearly not the case!  In fact, `\"ate the lion\"` is **more likely** than `\"ate the mouse`\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering how these probabilities are calculated, we can see why this is the case:\n",
    "    \n",
    "$p(ate\\_the\\_XXX) = p(ate) + p(ate|the) + p(XXX|the)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.834633, -0.913814)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_gram_lm.ngrams.prob((\"the\", \"lion\")), bi_gram_lm.ngrams.prob((\"the\", \"mouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the `\"lion\"` appeared **one time more than** `\"mouse\"`, this increased probability impacted our `3-gram` calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then, in that case, our `3-gram` language model should be able handle this problem better.  It would have modeled `\"ate the XXX\"` explicitly and would not, therefore, need to generate a probability by considering the smaller `n-gram`s that caused us problems above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_gram_lm = lm.ARPALanguageModel(\n",
    "    filename=\"resource_files/language_model/animal_lm-3_gram.iarpa\",\n",
    "    base_e=False  # this will keep the log probabilities in `base 10` so that they match up with the original file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so in this case, using `score()` will simply require looking up (`.ngrams.prob()`) the explicit `3-gram` that was captured by the language model explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.005498813623791417, 0.016496469180468848)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**tri_gram_lm.score((\"ate\", \"the\", \"lion\")), 10**tri_gram_lm.score((\"ate\", \"the\", \"mouse\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.05263153058738709, 0.15789486272078615)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**tri_gram_lm.ngrams.prob((\"ate\", \"the\", \"lion\")), 10**tri_gram_lm.ngrams.prob((\"ate\", \"the\", \"mouse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, our intuitions match.  `\"ate the lion\"` is **three times less likely** than `\"ate the mouse\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the higher the order of `n-gram` you are willing to model, the more \"accurate\" your language modeling will become.  But at a certain point, this becomes unwieldly.  You can see that our `librispeech` data comes with a `3-gram` **and** a `4-gram` model, but notice the size doubles just going from `3-gram`s to `4-gram`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 root root 725M Oct  3 20:30 3-gram.arpa.gz\n",
      "-rw-r--r--  1 root root 1.3G Oct  3 20:30 4-gram.arpa.gz\n",
      "lrwxr-xr-x  1 root root   14 Oct 20 20:07 lm_fglarge.arpa.gz -> 4-gram.arpa.gz\n",
      "lrwxr-xr-x  1 root root   14 Oct 20 20:07 lm_tglarge.arpa.gz -> 3-gram.arpa.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -lah raw_data/ | grep -E \"gram\\.arpa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by looking at the number of `n-gram`s modeled, this shouldn't be a surprise as there are ~60 million `4-gram`s in the `4-gram` language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\data\\\n",
      "ngram 1=200003\n",
      "ngram 2=38229161\n",
      "ngram 3=49712290\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n5 raw_data/3-gram.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\data\\\n",
      "ngram 1=200003\n",
      "ngram 2=38229161\n",
      "ngram 3=45941329\n",
      "ngram 4=60975692\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n6 raw_data/4-gram.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pruning` a language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where `pruning` can come in handy.  Since a good number of the total `n-gram`s that appear in a corpus are infrequent (remember, any `n-gram` seen **even once** has to be modeled), these can be removed from the language model without reducing the \"accuracy\" of the model too much.\n",
    "\n",
    "The `IRSTLM` manual (found in `resource_files/resources/irstlm-manual.pdf`) explains pruning this way:\n",
    "\n",
    "```\n",
    "Large LMs files can be pruned in a smart way by means of the command prune-lm that removes n-grams for which resorting to the back-off results in a small loss.\n",
    "```\n",
    "\n",
    "The `librispeech` data provides two `pruned` `3-gram` language models, each with a different `pruning` threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 root root 2.3G Oct  3 20:30 3-gram.arpa\n",
      "-rw-r--r--  1 root root  33M Oct  3 20:30 3-gram.pruned.1e-7.arpa.gz\n",
      "-rw-r--r--  1 root root  14M Oct  3 20:30 3-gram.pruned.3e-7.arpa.gz\n",
      "lrwxr-xr-x  1 root root   14 Oct 20 20:07 lm_tglarge.arpa.gz -> 3-gram.arpa.gz\n",
      "lrwxr-xr-x  1 root root   26 Oct 20 20:07 lm_tgmed.arpa.gz -> 3-gram.pruned.1e-7.arpa.gz\n",
      "lrwxr-xr-x  1 root root   26 Oct 20 20:07 lm_tgsmall.arpa.gz -> 3-gram.pruned.3e-7.arpa.gz\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -lh raw_data/ | grep 3-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the significant reduction in size.  This will be evident in the number of `2-gram`s and `3-gram`s as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\data\\\n",
      "ngram 1=200003\n",
      "ngram 2=38229161\n",
      "ngram 3=49712290\n",
      "...\n",
      "\n",
      "\\data\\\n",
      "ngram 1=200003\n",
      "ngram 2=2451827\n",
      "ngram 3=1134656\n",
      "...\n",
      "\n",
      "\\data\\\n",
      "ngram 1=200003\n",
      "ngram 2=1016673\n",
      "ngram 3=340026\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -n5 raw_data/3-gram.arpa\n",
    "echo ...\n",
    "head -n5 raw_data/3-gram.pruned.1e-7.arpa\n",
    "echo ...\n",
    "head -n5 raw_data/3-gram.pruned.3e-7.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's not necessary to `prune` our toy animal language models, it **is** easy to do with `IRSTLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "prune-lm - prunes language models\n",
      "\n",
      "USAGE:\n",
      "       prune-lm [options] <inputfile> [<outputfile>]\n",
      "\n",
      "DESCRIPTION:\n",
      "       prune-lm reads a LM in either ARPA or compiled format and\n",
      "       prunes out n-grams (n=2,3,..) for which backing-off to the\n",
      "       lower order n-gram results in a small difference in probability.\n",
      "       The pruned LM is saved in ARPA format\n",
      "\n",
      "OPTIONS:\n",
      "Parameters:\n",
      "    Help:      print this help\n",
      "    abs:      uses absolute value of weighted difference; default is 0\n",
      "    h:      print this help\n",
      "    t:      pruning thresholds for 2-grams, 3-grams, 4-grams,...; if less thresholds are specified, the last one is applied to all following n-gram levels; default is 0\n",
      "    threshold:      pruning thresholds for 2-grams, 3-grams, 4-grams,...; if less thresholds are specified, the last one is applied to all following n-gram levels; default is 0\n",
      "\n",
      "DEBUG_LEVEL:0/1 Everything OK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export IRSTLM=${KALDI_PATH}/tools/irstlm\n",
    "export PATH=${PATH}:${IRSTLM}/bin\n",
    "prune-lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You can provide a different threshold for each `n-gram`.  In the `librispeech` language models, you can see that no `pruning` was done on `1-gram`s."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
