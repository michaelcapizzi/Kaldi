{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2: Inspecting the `data` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_prepare_data.sh` will generate a new directory,`data`, that contains many of the files required for the `ASR` pipeline.  We will inspect its contents below.\n",
    "\n",
    "**Note:** The official `kaldi` documentation has a more detailed explanation of these files [here](http://kaldi-asr.org/doc/data_prep.html).  Just beware that **some** files explained there are not relevant to our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `data/{train|test}_dir`\n",
    "\n",
    "These directories contain four files for each of the subsets, `train` and `test` (assuming you set up a configuration with both when you ran `run_prepare_data.sh`, which we did)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spk2utt  text  utt2spk  wav.scp\n"
     ]
    }
   ],
   "source": [
    "ls data/train_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `wav.scp`\n",
    "\n",
    "This file maps each `audio basename` to its full path.  It takes the format:\n",
    "\n",
    "```\n",
    "[audio-basename] [full/path/to/audio]\n",
    "[audio-basename] [full/path/to/audio]\n",
    "[audio-basename] [full/path/to/audio]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "head -n5 data/train_dir/wav.scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `kaldi` allows the second argument of this file to be a `piped` command.  See this example from the official documentation:\n",
    "\n",
    "```\n",
    "s5# head -3 data/train/wav.scp\n",
    "sw02001-A /home/dpovey/kaldi-trunk/tools/sph2pipe_v2.5/sph2pipe -f wav -p -c 1 /export/corpora3/LDC/LDC97S62/swb1/sw02001.sph |\n",
    "sw02001-B /home/dpovey/kaldi-trunk/tools/sph2pipe_v2.5/sph2pipe -f wav -p -c 2 /export/corpora3/LDC/LDC97S62/swb1/sw02001.sph |\n",
    "```\n",
    "\n",
    "In this case `/home/dpovey/kaldi-trunk/tools/sph2pipe_v2.5/sph2pipe -f wav -p -c 1 /export/corpora3/LDC/LDC97S62/swb1/sw02001.sph` is a command that will convert a `.sph` file to `wav`.  The final character, `|`, acts as if it were `piping` the output to a file location.  \n",
    "\n",
    "In our case, we explicitly converted the audio as a preprocessing step, but one can easily follow this approach if it's important that the original audio file not be modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `text`\n",
    "\n",
    "This file is a subset of the `transcripts` file supplied to `run_prepare_data.sh` that contains **only** the transcripts of the utterances in that particular subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -n5 data/train_dir/text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `utt2spk`\n",
    "\n",
    "This file maps each utterance to a speaker.  Having speaker information becomes important later when we try to make certain speaker-level adaptations to the model (in an attempt to generalize the characteristics of a particular speaker). \n",
    "\n",
    "The file takes the following format:\n",
    "\n",
    "```\n",
    "[utterance-id] [speaker-id]\n",
    "[utterance-id] [speaker-id]\n",
    "[utterance-id] [speaker-id]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272-128104-0000 1272-128104-0000\n",
      "1272-128104-0001 1272-128104-0001\n",
      "1272-128104-0002 1272-128104-0002\n",
      "1272-128104-0003 1272-128104-0003\n",
      "1272-128104-0004 1272-128104-0004\n"
     ]
    }
   ],
   "source": [
    "head -n5 data/train_dir/utt2spk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If we weren't to have any information about speakers in our dataset, we could simply make the `speaker-id` the same as the `utterance-id`, which would result in `n` unique speakers (where `n` is the number of utterances in the subset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so you can see from the file, this `kaldi` pipeline works under the assumption that we do **not** have any information about speakers.  This is not particularly true in the case of the `librispeech` dataset, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the files created by an intermediate step during `1:1 Downloading...` created an `utt2spk` file for each of the original `librispeech` subsets.  They are stored in `raw_data/Librispeech/[subset]_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272-128104-0000 1272-128104\n",
      "1272-128104-0001 1272-128104\n",
      "1272-128104-0002 1272-128104\n",
      "1272-128104-0003 1272-128104\n",
      "1272-128104-0004 1272-128104\n"
     ]
    }
   ],
   "source": [
    "head -n5 raw_data/LibriSpeech/dev-clean_data/utt2spk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that the `utterance-id` convention is such that the last portion `000*` refers to a segment of a larger audio directory, all spoken by the same person.\n",
    "\n",
    "**Note:** We will stick with our current `utt2spk` for the sake of continuity in our pipeline, but at a later date, we can revisit this and see how performance changes (improves?) when using an `utt2spk` file that takes speaker information into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `spk2utt`\n",
    "\n",
    "This file is simply a reverse mapping of `utt2spk`.  Instead of mapping an uttreance to a speaker, this file maps a speaker to all of her or his spoken utterances.  It thus takes the format:\n",
    "\n",
    "```\n",
    "[speaker-id] [utterance-id_1] [utterance-id_2] [utterance-id_3] ... [utterance-id_n]\n",
    "[speaker-id] [utterance-id_1] [utterance-id_2] [utterance-id_3] ... [utterance-id_n]\n",
    "[speaker-id] [utterance-id_1] [utterance-id_2] [utterance-id_3] ... [utterance-id_n]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, this will be identicial to `utt2spk` since we are not taking speaker information into account (and we thus have a unique speaker for each utterance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272-128104-0000 1272-128104-0000\n",
      "1272-128104-0001 1272-128104-0001\n",
      "1272-128104-0002 1272-128104-0002\n",
      "1272-128104-0003 1272-128104-0003\n",
      "1272-128104-0004 1272-128104-0004\n"
     ]
    }
   ],
   "source": [
    "head -n5 data/train_dir/spk2utt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But looking at the `spk2utt` in `raw_data/LibriSpeech/[subset]`, we can see what a `spk2utt` would like that **does** take speaker information into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272-128104 1272-128104-0000 1272-128104-0001 1272-128104-0002 1272-128104-0003 1272-128104-0004 1272-128104-0005 1272-128104-0006 1272-128104-0007 1272-128104-0008 1272-128104-0009 1272-128104-0010 1272-128104-0011 1272-128104-0012 1272-128104-0013 1272-128104-0014\n",
      "1272-135031 1272-135031-0000 1272-135031-0001 1272-135031-0002 1272-135031-0003 1272-135031-0004 1272-135031-0005 1272-135031-0006 1272-135031-0007 1272-135031-0008 1272-135031-0009 1272-135031-0010 1272-135031-0011 1272-135031-0012 1272-135031-0013 1272-135031-0014 1272-135031-0015 1272-135031-0016 1272-135031-0017 1272-135031-0018 1272-135031-0019 1272-135031-0020 1272-135031-0021 1272-135031-0022 1272-135031-0023 1272-135031-0024\n"
     ]
    }
   ],
   "source": [
    "head -n2 raw_data/LibriSpeech/dev-clean_data/spk2utt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `segments` (optional)\n",
    "\n",
    "This file does **not** appear in our pipeline when using `librispeech` data, but if were accessing audio that was **not** already segmented, we would have supplied a master `segments` file to `run_prepare_data.sh`, which would have generated a `{train|test}`-specific `segments` file here.  It would take the following format:\n",
    "\n",
    "```\n",
    "[audio-basename] [utterance-id] [utterance-start] [utterance-stop]\n",
    "[audio-basename] [utterance-id] [utterance-start] [utterance-stop]\n",
    "[audio-basename] [utterance-id] [utterance-start] [utterance-stop]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `data/local`\n",
    "\n",
    "This directory is an intermediate (essentially, `temp`) directory used for housing files as they are manipulated and/or built for later use.  All the **important** files will appear in another subdirectory of `data`, so we won't spend too much time on the items here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `waves.{train|test}`\n",
    "\n",
    "These files are simply a list of the audio files that belong to the `train` and/or `test` subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head -n5 data/local/waves.train\n",
    "echo ...\n",
    "head -n5 data/local/waves.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lm_tg.arpa`\n",
    "\n",
    "This is a modified version of the `language model` that you supplied as an argument to `run_prepare_data.sh` with any `n-gram` containing `<UNK>` removed.  This will ensure that our model will **never** predict `<UNK>` when decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff data/local/lm_tg.arpa raw_data/3-gram.pruned.3e-7.arpa | head -n10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `data/local/dict`\n",
    "\n",
    "This directory contains files pertaining to the `lexicon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data/local/dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `lexicon.txt`\n",
    "\n",
    "This is just a local copy of the `lexicon` you supplied to `run_prepare_data.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff data/local/dict/lexicon.txt raw_data/librispeech-lexicon.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** `kaldi` will do this alot: it will **copy** files to a `local` location and then call those files from that **local** location (as opposed to their original locations.  This is inefficient from a disk space perspective in my opinion, but it would be a tremendous amount of work to parameterize all the scripts to take a location.  And disk space is cheap...So we will suffer it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `lexiconp.txt`\n",
    "\n",
    "This is a form of the `lexicon` with an additional value for each word: the probability of that pronunciation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/local/dict/lexiconp.txt | grep INDIRECTLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows for you to provide a `lexicon` that not only provides alternative pronunciations, but weights them according to their likelihood.  In our case, however, we don't have data to support the setting of those values, so all pronunciations are equally weighted at `1.0` (yes, it probably should be `.5` and `.5`, but `kaldi` is OK with `1.0` all the different pronunciations of a given word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `{non}silence_phones.txt` and `optional_silence.txt`\n",
    "\n",
    "`silence_phones.txt` and `nonsilence_phones.txt` simply separate the `phones` we supplied to `run_data_prepare.sh` into those that refer to `silence` and those that don't.  In our case, the only `silence` phone is `SIL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/local/dict/silence_phones.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optional_silence.txt` contains the value for a `phone` we will use to identify the `silence` between words.  The official `kaldi` documentation linked above doesn't go into much detail as to why this is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/local/dict/optional_silence.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `data/local/lang`\n",
    "\n",
    "This directory is truly only a `temp` `directory` used in the building of `data/lang`.  So we will skip investigating this directory, and move on to `data/lang`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `data/lang`\n",
    "\n",
    "This directory will contain all the files needed to utilize the `language model` (how `kaldi` accesses the `ARPA`-format `language model` during decoding will be discussed later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `words.txt`\n",
    "\n",
    "This file is a mapping of each word in our `lexicon` to an index.\n",
    "\n",
    "**Note:** `kaldi` does this often.  You will see many examples of files like this that map something to an index for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<eps> 0\n",
      "<unk> 1\n",
      "A 2\n",
      "A''S 3\n",
      "A'BODY 4\n",
      "...\n",
      "ZZZ 200000\n",
      "ZZZZ 200001\n",
      "#0 200002\n",
      "<s> 200003\n",
      "</s> 200004\n"
     ]
    }
   ],
   "source": [
    "head -n5 data/lang/words.txt\n",
    "echo ...\n",
    "tail -n5 data/lang/words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice a few added \"words\" to this list (more on each of these in later weeks):\n",
    "\n",
    " - `<eps>`: a faux-word used to model the \"space\" between words in a later step\n",
    " - `#0`: a faux-word used to allow our `finite state transducer (FST)` to function properly\n",
    " - `<s>`: an faux-word used to model the \"start of an utterance\" in a later step\n",
    " - `</s>`: an faux-word used to model the \"end of an utterance\" in a later step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `phones.txt`\n",
    "\n",
    "This file is a mapping of each phone (from the list that we supplied to `run_prepare_data.sh`) to an index.\n",
    "\n",
    "In this case, there are two additional differences between this file and the one we supplied to `run_prepare_data.sh`:\n",
    "\n",
    " - an additional `<eps>` phone is added (more about the role this plays later)\n",
    " - each phone is converted to `BIES` notation, which identifies where the phone occurs in the word \n",
    "   - `B`eginning\n",
    "   - `I`nside\n",
    "   - `E`nd\n",
    "   - `S`olo (the word is made up of only this phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<eps> 0\n",
      "SIL 1\n",
      "SIL_B 2\n",
      "SIL_E 3\n",
      "SIL_I 4\n",
      "SIL_S 5\n",
      "AA0_B 6\n",
      "AA0_E 7\n",
      "AA0_I 8\n",
      "AA0_S 9\n"
     ]
    }
   ],
   "source": [
    "head data/lang/phones.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `{arpa}_oov.txt`\n",
    "\n",
    "These two files identify any words in the `lexicon` that do **not** appear in our `language model` (`OOV` = out of vocabulary).\n",
    "\n",
    "**Note:** This will hopefully become evident later, but this would cause a significant problem for our `ASR` model, and so we took steps during the preparation stage to identify any such words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK>\n",
      "...\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "cat data/lang/arpa_oov.txt\n",
    "echo ...\n",
    "cat data/lang/oov.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, since our `language model` and `lexicon` were both built directly from the same source, it shouldn't be surprising to see an \"empty\" list.  But you can imagine a situation where your `language model` would be built from a very large corpus, and while there are ways to automatically generate (approximate) pronunciations of words (see [here](https://www-i6.informatik.rwth-aachen.de/web/Software/g2p.html) for one such program), it may not be an efficient step in the data preparation to worry about ensuring all words are present in both files.\n",
    "\n",
    "In a later step, we will automatically **remove** any such `OOV` words from the final `FST`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `oov.int`\n",
    "\n",
    "This file is a representation of `oov.txt` using indices instead of strings.\n",
    "\n",
    "**Note:** `kaldi` will do this a lot.  And the filetypes (`.txt` v. `int`) will be an indication of which form to expect.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "cat data/lang/oov.int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `topo`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `L.fst` and `L_disambig.fst`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: fstinfo: command not found\n"
     ]
    },
    {
     "ename": "",
     "evalue": "127",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "fstinfo --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prints out information about an FST.\n",
      "\n",
      "  Usage: /home/kaldi/tools/openfst/bin/fstinfo [in.fst]\n",
      "\n",
      "PROGRAM FLAGS:\n",
      "\n",
      "  --arc_filter: type = string, default = \"any\"\n",
      "  Arc filter: one of: \"any\", \"epsilon\", \"iepsilon\", \"oepsilon\"; this only affects the counts of (co)accessible states, connected states, and (strongly) connected components\n",
      "  --fst_verify: type = bool, default = true\n",
      "  Verify FST sanity\n",
      "  --info_type: type = string, default = \"auto\"\n",
      "  Info format: one of: \"auto\", \"long\", \"short\"\n",
      "  --pipe: type = bool, default = false\n",
      "  Send info to stderr, input to stdout\n",
      "  --test_properties: type = bool, default = true\n",
      "  Compute property values (if unknown to FST)\n",
      "\n",
      "LIBRARY FLAGS:\n",
      "\n",
      "Flags from: flags.cc\n",
      "  --help: type = bool, default = false\n",
      "  show usage information\n",
      "  --helpshort: type = bool, default = false\n",
      "  show brief usage information\n",
      "  --tmpdir: type = string, default = \"/tmp\"\n",
      "  temporary directory\n",
      "  --v: type = int32, default = 0\n",
      "  verbosity level\n",
      "\n",
      "Flags from: fst.cc\n",
      "  --fst_align: type = bool, default = false\n",
      "  Write FST data aligned where appropriate\n",
      "  --fst_default_cache_gc: type = bool, default = true\n",
      "  Enable garbage collection of cache\n",
      "  --fst_default_cache_gc_limit: type = int64, default = 1048576\n",
      "  Cache byte size that triggers garbage collection\n",
      "  --fst_read_mode: type = string, default = \"read\"\n",
      "  Default file reading mode for mappable files\n",
      "  --fst_verify_properties: type = bool, default = false\n",
      "  Verify FST properties queried by TestProperties\n",
      "  --save_relabel_ipairs: type = string, default = \"\"\n",
      "  Save input relabel pairs to file\n",
      "  --save_relabel_opairs: type = string, default = \"\"\n",
      "  Save output relabel pairs to file\n",
      "\n",
      "Flags from: symbol-table.cc\n",
      "  --fst_compat_symbols: type = bool, default = true\n",
      "  Require symbol tables to match when appropriate\n",
      "  --fst_field_separator: type = string, default = \"\t \"\n",
      "  Set of characters used as a separator between printed fields\n",
      "\n",
      "Flags from: util.cc\n",
      "  --fst_error_fatal: type = bool, default = true\n",
      "  FST errors are fatal; o.w. return objects flagged as bad: e.g., FSTs: kError property set, FST weights: not a Member()\n",
      "\n",
      "Flags from: weight.cc\n",
      "  --fst_weight_parentheses: type = string, default = \"\"\n",
      "  Characters enclosing the first weight of a printed composite weight (e.g., pair weight, tuple weight and derived classes) to ensure proper I/O of nested composite weights; must have size 0 (none) or 2 (open and close parenthesis)\n",
      "  --fst_weight_separator: type = string, default = \",\"\n",
      "  Character separator between printed composite weights; must be a single character\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "/home/kaldi/tools/openfst/bin/fstinfo --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `data/lang/phones`\n",
    "\n",
    "This directory..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
