{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4: Inspecting `HCLG`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebooks we ran `run_compile_graph.sh` which built a large `FST` called `HCLG.fst`.  And since we ran this command for **each** of the three acoustic models we built, you will see an `HCLG.fst` in each of the three \"main\" subdirectories of `exp`: `monophones`, `triphones`, and `triphones_lda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -lah exp/monophones | grep HCLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -lah exp/triphones | grep HCLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls -lah exp/triphones_lda | grep HCLG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you should notice is that, perhaps not surprisingly, the `HCLG` is larger for each of the subsequent levels of the acoustic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we inspect the `HCLG` in more detail, take the time to read [this **excellent** blog post](http://vpanayotov.blogspot.com/2012/06/kaldi-decoding-graph-construction.html) (from one of the main contributors of `kaldi`) that describes in detail the makeup of the `HCLG`.  \n",
    "\n",
    "You may also want to read **Section 6** of [this tutorial](https://github.com/michaelcapizzi/kaldi/blob/kaldi_instructional/egs/INSTRUCTIONAL/resource_files/resources/wfst_tutorial.pdf) which is a summary of the [original paper](https://github.com/michaelcapizzi/kaldi/blob/kaldi_instructional/egs/INSTRUCTIONAL/resource_files/resources/wfst_paper.pdf) introducing the idea of an `HCLG`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building `HCLG`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `HCLG` is a single `FST` that represents **all** aspects of our `ASR` pipeline into one graph.  As you read, it is the `composition` of **four** separate `FST`s (Note that they are `composed` in reverse order.\n",
    "\n",
    "Decoding (when we take a new audio file and predict what was said) comes down to two steps:\n",
    "\n",
    "   1. determining which `GMM` best matches the incoming frames\n",
    "   2. looking for the most likely path through the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same packages we used last week to take a closer look at `HCLG` and its composites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because of the way `kaldi` installed `openFST` we have to add the path to the python functions here\n",
    "import sys\n",
    "sys.path.append(\"/scratch/kaldi/tools/openfst-1.6.2/lib/python2.7/site-packages\")    \n",
    "\n",
    "from utils.fst_manipulate import fst_manipulate as fstman  # scripts to further manipulate fsts\n",
    "\n",
    "import pywrapfst as openfst  # the wrapper module\n",
    "import graphviz as dot       # a wrapper for graphviz, which will allow us to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `G`\n",
    "\n",
    "`G` is the `FST` representation of our `language model`.  We looked at this in detail in the last week's notebooks.  It has `word:word` on its edges.  Below is our `language model` built from the animal corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FstIOError",
     "evalue": "Read failed: 'resource_files/fst/animal_fst-2_gram.fst'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFstIOError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b0033921adef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG_animal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenfst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resource_files/fst/animal_fst-2_gram.fst\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mG_animal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpywrapfst.pyx\u001b[0m in \u001b[0;36mpywrapfst.Fst.read (pywrapfst.cc:29673)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpywrapfst.pyx\u001b[0m in \u001b[0;36mpywrapfst._read_Fst (pywrapfst.cc:28985)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFstIOError\u001b[0m: Read failed: 'resource_files/fst/animal_fst-2_gram.fst'"
     ]
    }
   ],
   "source": [
    "G_animal = openfst.Fst.read(\"resource_files/fst/animal_fst-2_gram.fst\")\n",
    "G_animal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"real\" `G.fst` can be found in `data/lang` and was built way back in week 3 when we ran `run_prepare_data.sh` to build the `data` directory.\n",
    "\n",
    "**Note:** These composite `FST`s will be too big to visualize, but we can still gather some information about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = openfst.Fst.read(\"data/lang_test_tg/G.fst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the number of states by converting the `iterator` in `.states()` to a `<list>` and getting its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1178044"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_states = len(list(G.states()))\n",
    "G_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a similar thing with the `.arcs()` `iterator` with a small modification.  `.arcs()` takes one argument, a `state`, and so we loop through all `state`s, and then count how many `arc`s that state has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2659792"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_arcs = 0\n",
    "for s in G.states():\n",
    "    intermediate_arcs = len(list(G.arcs(s)))\n",
    "    G_arcs += intermediate_arcs\n",
    "G_arcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get some information about the `arc`s.\n",
    "\n",
    "**Note:** We'll just look at the first **five** states and then `break` out of our nested `for` loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: 2\n",
      "out: 2\n",
      "weight: 4.15409374\n",
      "=========\n",
      "in: 49\n",
      "out: 49\n",
      "weight: 10.8683262\n",
      "=========\n",
      "in: 220\n",
      "out: 220\n",
      "weight: 10.9160538\n",
      "=========\n",
      "in: 227\n",
      "out: 227\n",
      "weight: 10.9750328\n",
      "=========\n",
      "in: 456\n",
      "out: 456\n",
      "weight: 7.23240852\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for s in G.states():\n",
    "    for a in G.arcs(s):\n",
    "        if c < 5:\n",
    "            print(\"in: {}\\nout: {}\\nweight: {}\".format(\n",
    "                a.ilabel,\n",
    "                a.olabel,\n",
    "                a.weight\n",
    "                )\n",
    "            )\n",
    "            print(\"=========\")\n",
    "        else:\n",
    "            break\n",
    "        c += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that `openfst` will use `index`es instead of `string`s on the `arc`s, but we can recover the words by looking at the \"lookup\" file `kaldi` built for this very purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 2\n",
      "AARON 49\n",
      "ABE 220\n",
      "ABEL 227\n",
      "ABOUT 456\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat data/lang/words.txt | grep -E \" 2$\"       # the -E flag will allow us to use a regex\n",
    "cat data/lang/words.txt | grep -E \" 49$\"\n",
    "cat data/lang/words.txt | grep -E \" 220$\"\n",
    "cat data/lang/words.txt | grep -E \" 227$\"\n",
    "cat data/lang/words.txt | grep -E \" 456$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the `ilabel` and `olabel` are the same, confirming that the `arc`s consist of `word:word`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `L`\n",
    "`L` is the `FST` representation of our `lexicon`.  This was built in week 3 when we built the `data` directory (see `3.2: Inspecting data dir`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root  37M Feb  1 21:00 L.fst\r\n",
      "-rw-r--r-- 1 root root  38M Feb  1 21:00 L_disambig.fst\r\n"
     ]
    }
   ],
   "source": [
    "ls -lah data/lang/ | grep L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = openfst.Fst.read(\"data/lang/L.fst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: 0\n",
      "out: 0\n",
      "weight: 0.693147182\n",
      "=========\n",
      "in: 1\n",
      "out: 0\n",
      "weight: 0.693147182\n",
      "=========\n",
      "in: 5\n",
      "out: 1\n",
      "weight: 0.693147182\n",
      "=========\n",
      "in: 5\n",
      "out: 1\n",
      "weight: 0.693147182\n",
      "=========\n",
      "in: 33\n",
      "out: 2\n",
      "weight: 0.693147182\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for s in L.states():\n",
    "    for a in L.arcs(s):\n",
    "        if c < 5:\n",
    "            print(\"in: {}\\nout: {}\\nweight: {}\".format(\n",
    "                a.ilabel,\n",
    "                a.olabel,\n",
    "                a.weight\n",
    "                )\n",
    "            )\n",
    "            print(\"=========\")\n",
    "        else:\n",
    "            break\n",
    "        c += 1\n",
    "    if c > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`L` has a different `arc` structure, consisting of `letter:word`.  And so to understand these `arc`s we need to also access the `phones` \"lookup\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
